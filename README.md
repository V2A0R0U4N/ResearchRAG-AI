# üìò ResearchRAG-AI ‚Äî Advanced AI Research Assistant

> An intelligent, document-grounded RAG (Retrieval-Augmented Generation) system that answers questions **only** from the documents you provide ‚Äî with **zero hallucination**.

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-1.x-red?logo=streamlit)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-green)
![Groq](https://img.shields.io/badge/LLM-Groq%20Llama%203.1-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## ‚ú® Features

| Feature | Description |
|---|---|
| üîó **Multi-Source Ingestion** | Load from URLs, PDFs, DOCX, and TXT files simultaneously |
| üîÄ **Hybrid Search** | BM25 (keyword) + FAISS (semantic) via `EnsembleRetriever` |
| üß† **6-Layer Anti-Hallucination** | Pipeline to detect and block fabricated answers |
| ü§ñ **LLM Intent Classifier** | AI-powered check to reject nonsensical/off-topic questions |
| üìö **Source Citations** | Every answer cites the exact document/URL it came from |
| ‚òÅÔ∏è **Cloud + Local LLM** | Switch between Groq API (cloud) and Ollama (local) |

---

## üõ°Ô∏è Anti-Hallucination Pipeline (6 Layers)

```
User Query
    ‚Üì
Layer 1    ‚Üí Regex Guard         (blocks greetings, slurs, identity questions)
    ‚Üì
Layer 1.5  ‚Üí LLM Intent Check    (RESEARCH / MIXED / NONSENSE classifier)
    ‚Üì
Layer 2    ‚Üí FAISS Relevance Gate (rejects if no document chunk is close enough)
    ‚Üì
Layer 3    ‚Üí Hybrid Retrieval    (60% FAISS semantic + 40% BM25 keyword)
    ‚Üì
Layer 4    ‚Üí Strict Prompt       (LLM forbidden from using external knowledge)
    ‚Üì
Layer 5    ‚Üí Post-Processing     (catches any "no info" leakage in LLM response)
    ‚Üì
    ‚úÖ Grounded Answer with Citations
```

---

## ü§ñ LLM Providers

### ‚òÅÔ∏è Cloud ‚Äî Groq API (Llama 3.1 8B Instant)
- **Get your free API key:** [https://console.groq.com/home](https://console.groq.com/home)
- Fast inference, 14,400 requests/day free tier
- Model: `llama-3.1-8b-instant`

### üñ•Ô∏è Local ‚Äî Ollama (Llama 3.1)
- **Install Ollama:** [https://ollama.ai](https://ollama.ai)
- Pull the model: `ollama pull llama3.1`
- Runs 100% offline, no API key needed

---

## üöÄ Setup & Run

### 1. Clone the repository
```bash
git clone https://github.com/V2A0R0U4N/ResearchRAG-AI.git
cd ResearchRAG-AI
```

### 2. Create a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate        # Mac/Linux
# venv\Scripts\activate         # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up your API key
Create a `.env` file in the project root:
```env
GROQ_API_KEY=your_groq_api_key_here
OLLAMA_BASE_URL=http://localhost:11434    # optional, only for local Ollama
```
Get your Groq API key for free at: [https://console.groq.com/home](https://console.groq.com/home)

### 5. Run the app
```bash
streamlit run main.py
```

---

## üì¶ Requirements

```
streamlit
python-dotenv
langchain>=0.1.17
langchain-community>=0.0.25
langchain-openai>=0.0.6
langchain-huggingface>=0.0.3
langchain-groq>=0.1.0
langchain-ollama>=0.1.0
langchain-groq>=0.1.0
faiss-cpu
sentence-transformers
unstructured
pypdf>=3.0.0
docx2txt>=0.8
rank_bm25>=0.2.2
requests
tqdm
beautifulsoup4
```

---

## üìÅ Project Structure

```
ResearchRAG-AI/
‚îú‚îÄ‚îÄ main.py              # Streamlit app ‚Äî full RAG pipeline
‚îú‚îÄ‚îÄ query_guard.py       # Layer 1 regex guard + Layer 2 relevance gate
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .env                 # API keys (not committed to git)
‚îú‚îÄ‚îÄ .gitignore           # Excludes venv, .env, .pkl files
‚îî‚îÄ‚îÄ README.md
```

---

## üß™ How to Use

1. **Add sources** ‚Äî Paste article URLs (one per field) or upload PDF/DOCX/TXT files
2. **Click "Process Research Material"** ‚Äî Documents are chunked, embedded, and indexed
3. **Ask a question** ‚Äî Type a specific research question about your documents
4. **Get a grounded answer** ‚Äî The system retrieves relevant chunks and generates a cited response

> **Note:** The system will politely decline if your question is off-topic, nonsensical, or not covered in the provided documents.

---

## üîß Configuration

| Parameter | Default | Description |
|---|---|---|
| LLM Temperature | `0.1` | Low for factual accuracy |
| Chunk Size | `600` chars | Document splitting size |
| Chunk Overlap | `150` chars | Context continuity |
| Retrieval k | `8` | Chunks fetched per query |
| FAISS Threshold | `1.8` | L2 distance cutoff for relevance |
| Hybrid Weights | `60/40` | FAISS semantic / BM25 keyword |

---

## ‚ö†Ô∏è Important Notes

- The `.env` file with your API keys is **never** committed to GitHub (protected by `.gitignore`)
- The **Local (Ollama)** option will not work on Streamlit Community Cloud ‚Äî use Cloud (Groq) for deployments
- After restarting the app, you need to **re-process** your documents (the vector index is rebuilt in memory)

---

## üìÑ License

MIT License ‚Äî free to use, modify, and distribute.
